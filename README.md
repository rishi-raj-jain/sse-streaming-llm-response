![](https://upstash.com/blog/sse-streaming-llm-responses/opengraph-image)

# Using Server-Sent Events (SSE) to stream LLM responses in Next.js

<p>
  <a href="#introduction"><strong>Introduction</strong></a> ·
  <a href="#tech-stack--features"><strong>Tech Stack + Features</strong></a> ·
  <a href="#author"><strong>Author</strong></a>
</p>

## Introduction

Learn how to build use Server-Sent Events (SSE) to stream LLM responses in Next.js with OpenAI.

## Tech Stack + Features

### Frameworks

- [Next.js](https://nextjs.oth) – The React Framework for the Web.

- [Vercel AI SDK](https://sdk.vercel.ai/docs) – An open source library for building AI-powered user interfaces.

### Database

- [Upstash](https://upstash.com) - Serverless database platform. We're going to use Upstash Redis to cache OpenAI API responses.

### Artificial Intelligence

- [OpenAI](https://openai.com) - OpenAI is an artificial intelligence research lab focused on developing advanced AI technologies.

- [LangChain](https://js.langchain.com) - Framework for developing applications powered by language models.

### UI

- [TailwindCSS](https://tailwindcss.com) – A CSS framework for rapid and responsive styling.

### Platofrms

- [Vercel](https://www.vercel.com) - A cloud platform for deploying and scaling web applications.

## Author

- Rishi Raj Jain ([@rishi_raj_jain_](https://twitter.com/rishi_raj_jain_))
